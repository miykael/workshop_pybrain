{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<h3>Disclaimer</h3>\n",
    "The package <code>nistats</code> will soon be merged into <code>nilearn</code> and all of its functionality will be available in the release of <code>nilearn</code> 0.7.0 in late 2020. Instead of using the retired version of <code>nistats</code>, we decided to already provide a brief spoiler of how things look in the new version (by installing <code>nilearn</code> from the current <code>main branch</code>. While drastic changes regarding functions and modules shouldn't be a thin, please watch out for smaller differences like <code>arguments</code>, <code>function/argument names/defaults</code>, etc. . \n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "Nilearn GLM: statistical analyses of MRI in Python\n",
    "=========================================================\n",
    "\n",
    "[Nilearn]()'s [GLM/stats]() module allows fast and easy MRI statistical analysis.\n",
    "\n",
    "It leverages [Nibabel]() and other Python libraries from the Python scientific stack like [Scipy](), [Numpy]() and [Pandas]().\n",
    "\n",
    "In this tutorial, we're going to explore `nilearn's GLM` functionality by analyzing 1) a single subject single run and 2) three subject group level example using a General Linear Model (GLM). We're gonna use the same example dataset (ds000114) as from the `nibabel` and `nilearn` tutorials. As this is a multi run multi task dataset, we've to decide on a run and a task we want to analyze. Let's go with `ses-test` and `task-fingerfootlips`, starting with a single subject `sub-01`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual level analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting and inspecting the data\n",
    "=========================\n",
    "\n",
    "At first, we have to set and indicate the data we want to analyze. As stated above, we're going to use the anatomical image and the preprocessed functional image of `sub-01` from `ses-test`. The preprocessing was conducted through [fmriprep](https://fmriprep.readthedocs.io/en/stable/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_img = '/data/ds000114/derivatives/fmriprep/sub-01/ses-test/func/sub-01_ses-test_task-fingerfootlips_space-MNI152nlin2009casym_desc-preproc_bold.nii.gz'\n",
    "anat_img = '/data/ds000114/sub-01/ses-test/anat/sub-01_ses-test_T1w.nii.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the mean functional image and the subject's anatomy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.image import mean_img\n",
    "mean_img = mean_img(fmri_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_stat_map, plot_anat, plot_img, show, plot_glass_brain\n",
    "plot_img(mean_img)\n",
    "plot_anat(anat_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying the experimental paradigm\n",
    "------------------------------------\n",
    "\n",
    "We must now provide a description of the experiment, that is, define the\n",
    "timing of the task and rest periods. This is typically\n",
    "provided in an events.tsv file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "events = pd.read_table('/data/ds000114/sub-01/ses-test/func/sub-01_ses-test_task-fingerfootlips_events.tsv')\n",
    "print(events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing the GLM analysis\n",
    "---------------------------\n",
    "\n",
    "It is now time to create and estimate a ``FirstLevelModel`` object, that will generate the *design matrix* using the  information provided by the ``events`` object.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.glm.first_level import FirstLevelModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of important parameters one needs to define within a `FirstLevelModel` and the majority of them will have a prominent influence on your results. Thus, make sure to check them before running your model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FirstLevelModel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the TR of the functional images, luckily we can extract that information using `nibabel`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nib-ls /data/ds000114/derivatives/fmriprep/sub-01/ses-test/func/sub-01_ses-test_task-fingerfootlips_space-MNI152nlin2009casym_desc-preproc_bold.nii.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the `TR` is 2.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_glm = FirstLevelModel(t_r=2.5,\n",
    "                           noise_model='ar1',\n",
    "                           hrf_model='spm',\n",
    "                           drift_model='cosine',\n",
    "                           high_pass=1./160,\n",
    "                           signal_scaling=False,\n",
    "                           minimize_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, we also want to include confounds computed during preprocessing (e.g., motion, global signal, etc.) as regressors of no interest. In our example, these were computed by `fmriprep` and can be found in `derivatives/fmriprep/sub-01/func/`. We can use `pandas` to inspect that file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "confounds = pd.read_csv('/data/ds000114/derivatives/fmriprep/sub-01/ses-test/func/sub-01_ses-test_task-fingerfootlips_bold_desc-confounds_timeseries.tsv', delimiter='\\t')\n",
    "confounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparable to other neuroimaging softwards, we have a timepoint x confound dataframe. However, `fmriprep` computes way more confounds than most of you are used to and that require a bit of reading to understand and therefore utilize properly. We therefore and for the sake of simplicity stick to the \"classic\" ones: `WhiteMatter`, `GlobalSignal`, `FramewiseDisplacement` and the `motion correction parameters` in translation and rotation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "confounds_glm = confounds[['WhiteMatter', 'GlobalSignal', 'FramewiseDisplacement', 'X', 'Y', 'Z', 'RotX', 'RotY', 'RotZ']].replace(np.nan, 0)\n",
    "confounds_glm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have specified the model, we can run it on the fMRI image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_glm = fmri_glm.fit(fmri_img, events, confounds_glm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can inspect the design matrix (rows represent time, and\n",
    "columns contain the predictors).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_matrix = fmri_glm.design_matrices_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, we have taken the first design matrix, because the model is\n",
    "implictily meant to for multiple runs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_design_matrix\n",
    "plot_design_matrix(design_matrix)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the design matrix image to disk, first creating a directory where you want to write the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "outdir = 'results'\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)\n",
    "\n",
    "from os.path import join\n",
    "plot_design_matrix(design_matrix, output_file=join(outdir, 'design_matrix.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column contains the expected reponse profile of regions which are\n",
    "sensitive to the \"Finger\" task. Let's plot this first column:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(design_matrix['Finger'])\n",
    "plt.xlabel('scan')\n",
    "plt.title('Expected Response for condition \"Finger\"')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting voxels with significant effects\n",
    "-----------------------------------------\n",
    "\n",
    "To access the estimated coefficients (Betas of the GLM model), we\n",
    "created constrast with a single '1' in each of the columns: The role\n",
    "of the contrast is to select some columns of the model --and\n",
    "potentially weight them-- to study the associated statistics. So in\n",
    "a nutshell, a contrast is a weigted combination of the estimated\n",
    "effects.  Here we can define canonical contrasts that just consider\n",
    "the two condition in isolation ---let's call them \"conditions\"---\n",
    "then a contrast that makes the difference between these conditions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "conditions = {\n",
    "    'active - Finger': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
    "    'active - Foot':   array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
    "    'active - Lips':   array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at it: plot the coefficients of the contrast, indexed by\n",
    "the names of the columns of the design matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_contrast_matrix\n",
    "plot_contrast_matrix(conditions['active - Finger'], design_matrix=design_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we compute the estimated effect. It is in BOLD signal unit,\n",
    "but has no statistical guarantees, because it does not take into\n",
    "account the associated variance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_map = fmri_glm.compute_contrast(conditions['active - Finger'],\n",
    "                                    output_type='effect_size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get statistical significance, we form a t-statistic, and\n",
    "directly convert is into z-scale. The z-scale means that the values\n",
    "are scaled to match a standard Gaussian distribution (mean=0,\n",
    "variance=1), across voxels, if there were now effects in the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_map = fmri_glm.compute_contrast(conditions['active - Finger'],\n",
    "                                  output_type='z_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot thresholded z scores map.\n",
    "\n",
    "We display it on top of the average\n",
    "functional image of the series (could be the anatomical image of the\n",
    "subject).  We use arbitrarily a threshold of 3.0 in z-scale. We'll\n",
    "see later how to use corrected thresholds.  we show to display 3\n",
    "axial views: display_mode='z', cut_coords=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stat_map(z_map, bg_img=mean_img, threshold=3.0,\n",
    "              display_mode='z', cut_coords=3, black_bg=True,\n",
    "              title='active - Finger (Z>3)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_glass_brain(z_map, threshold=3.0, black_bg=True, plot_abs=False,\n",
    "                 title='active - Finger (Z>3)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical signifiance testing. One should worry about the\n",
    "statistical validity of the procedure: here we used an arbitrary\n",
    "threshold of 3.0 but the threshold should provide some guarantees on\n",
    "the risk of false detections (aka type-1 errors in statistics). One\n",
    "first suggestion is to control the false positive rate (fpr) at a\n",
    "certain level, e.g. 0.001: this means that there is.1% chance of\n",
    "declaring active an inactive voxel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.glm.thresholding import threshold_stats_img\n",
    "_, threshold = threshold_stats_img(z_map, alpha=.001, height_control='fpr')\n",
    "print('Uncorrected p<0.001 threshold: %.3f' % threshold)\n",
    "plot_stat_map(z_map, bg_img=mean_img, threshold=threshold,\n",
    "              display_mode='z', cut_coords=3, black_bg=True,\n",
    "              title='active - Finger (p<0.001)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_glass_brain(z_map, threshold=threshold, black_bg=True, plot_abs=False,\n",
    "                 title='active - Finger (p<0.001)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that with this you expect 0.001 * n_voxels to show up\n",
    "while they're not active --- tens to hundreds of voxels. A more\n",
    "conservative solution is to control the family wise errro rate,\n",
    "i.e. the probability of making ony one false detection, say at\n",
    "5%. For that we use the so-called Bonferroni correction:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, threshold = threshold_stats_img(z_map, alpha=.05, height_control='bonferroni')\n",
    "print('Bonferroni-corrected, p<0.05 threshold: %.3f' % threshold)\n",
    "plot_stat_map(z_map, bg_img=mean_img, threshold=threshold,\n",
    "              display_mode='z', cut_coords=3, black_bg=True,\n",
    "              title='active - Finger (p<0.05, corrected)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_glass_brain(z_map, threshold=threshold, black_bg=True, plot_abs=False,\n",
    "                 title='active - Finger (p<0.05, corrected)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite conservative indeed !  A popular alternative is to\n",
    "control the false discovery rate, i.e. the expected proportion of\n",
    "false discoveries among detections. This is called the false\n",
    "disovery rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, threshold = threshold_stats_img(z_map, alpha=.05, height_control='fdr')\n",
    "print('False Discovery rate = 0.05 threshold: %.3f' % threshold)\n",
    "plot_stat_map(z_map, bg_img=mean_img, threshold=threshold,\n",
    "              display_mode='z', cut_coords=3, black_bg=True,\n",
    "              title='active - Finger (fdr=0.05)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_glass_brain(z_map, threshold=threshold, black_bg=True, plot_abs=False,\n",
    "                 title='active - Finger (fdr=0.05)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally people like to discard isolated voxels (aka \"small\n",
    "clusters\") from these images. It is possible to generate a\n",
    "thresholded map with small clusters removed by providing a\n",
    "cluster_threshold argument. here clusters smaller than 10 voxels\n",
    "will be discarded.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_map, threshold = threshold_stats_img(\n",
    "    z_map, alpha=.05, height_control='fdr', cluster_threshold=10)\n",
    "plot_stat_map(clean_map, bg_img=mean_img, threshold=threshold,\n",
    "              display_mode='z', cut_coords=3, black_bg=True, colorbar=False,\n",
    "              title='active - Finger (fdr=0.05), clusters > 10 voxels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_glass_brain(z_map, threshold=threshold, black_bg=True, plot_abs=False,\n",
    "                 title='active - Finger (fdr=0.05), clusters > 10 voxels)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the effect and zscore maps to the disk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_map.to_filename(join(outdir, 'sub-01_ses-test_task-footfingerlips_space-MNI152nlin2009casym_desc-finger_zmap.nii.gz'))\n",
    "eff_map.to_filename(join(outdir, 'sub-01_ses-test_task-footfingerlips_space-MNI152nlin2009casym_desc-finger_effmap.nii.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the found positions in a table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.reporting import get_clusters_table\n",
    "table = get_clusters_table(z_map, stat_threshold=threshold,\n",
    "                           cluster_threshold=20)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table can be saved for future use:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.to_csv(join(outdir, 'table.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or use [atlasreader](https://github.com/miykael/atlasreader) to get even more information and informative figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from atlasreader import create_output\n",
    "from os.path import join\n",
    "z_map.to_filename(join(outdir, 'active_finger_z_map.nii.gz'))\n",
    "create_output(join(outdir, 'active_finger_z_map.nii.gz'),\n",
    "              cluster_extent=5, voxel_thresh=threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the csv file containing relevant information about the peak of each cluster. This table contains the cluster association and location of each peak, its signal value at this location, the cluster extent (in mm, not in number of voxels), as well as the membership of each peak, given a particular atlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_info = pd.read_csv('results/active_finger_z_map_peaks.csv')\n",
    "peak_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cluster_info = pd.read_csv('results/active_finger_z_map_clusters.csv')\n",
    "cluster_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each cluster, we also get a corresponding visualization, saved as `.png`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"results/active_finger_z_map.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"results/active_finger_z_map_cluster01.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"results/active_finger_z_map_cluster02.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"results/active_finger_z_map_cluster03.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But wait, there's more! There's even a functionality to create entire `GLM reports` including information regarding the `model` and its `parameters`, `design matrix`, `contrasts`, etc. . All we need is the `make_glm_report` function from `nilearn.reporting` and apply it to our `fitted GLM`, specifying a `contrast of interest`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.reporting import make_glm_report\n",
    "\n",
    "report = make_glm_report(fmri_glm,\n",
    "                         contrasts='Finger',\n",
    "                         bg_img=mean_img\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once generated, we have several options to view the `GLM report`: directly in the `notebook`, in the `browser` or save it as an `html` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report\n",
    "#report.open_in_browser()\n",
    "#report.save_as_html(\"GLM_report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing an F-test\n",
    "\n",
    "\"active vs rest\" is a typical t test: condition versus\n",
    "baseline. Another popular type of test is an F test in which one\n",
    "seeks whether a certain combination of conditions (possibly two-,\n",
    "three- or higher-dimensional) explains a significant proportion of\n",
    "the signal.  Here one might for instance test which voxels are well\n",
    "explained by combination of the active and rest condition.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "effects_of_interest = np.vstack((conditions['active - Finger'], conditions['active - Lips']))\n",
    "plot_contrast_matrix(effects_of_interest, design_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the contrast and compute the correspoding map. Actually, the\n",
    "contrast specification is done exactly the same way as for t\n",
    "contrasts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_map = fmri_glm.compute_contrast(effects_of_interest,\n",
    "                                  output_type='z_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the statistic has been converted to a z-variable, which\n",
    "makes it easier to represent it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_map, threshold = threshold_stats_img(\n",
    "    z_map, alpha=.05, height_control='fdr', cluster_threshold=0)\n",
    "plot_stat_map(clean_map, bg_img=mean_img, threshold=threshold,\n",
    "              display_mode='z', cut_coords=3, black_bg=True,\n",
    "              title='Effects of interest (fdr=0.05), clusters > 10 voxels', cmap='magma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating models\n",
    "While not commonly done, it's a very good and important idea to actually evaluate your model in terms of its fit. We can do that comprehensively, yet easily through `nilearn` functionality. In more detail, we're going to inspect the residuals and evaluate the predicted time series. Let's  do this for the peak voxels. At first, we have to extract them using `get_clusters_table`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = get_clusters_table(z_map, stat_threshold=1,\n",
    "                           cluster_threshold=20).set_index('Cluster ID', drop=True)\n",
    "table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this `dataframe`, we get the `largest clusters` and prepare a `masker` to extract their `time series`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import input_data\n",
    "\n",
    "# get the largest clusters' max x, y, and z coordinates\n",
    "coords = table.loc[range(1, 7), ['X', 'Y', 'Z']].values\n",
    "\n",
    "# extract time series from each coordinate\n",
    "masker = input_data.NiftiSpheresMasker(coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get and check model residuals\n",
    "We can simply obtain the `residuals` of the peak voxels from our `fitted model` via applying the prepared `masker` (and thus `peak voxel`) to the `residuals` our:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = masker.fit_transform(fmri_glm.residuals[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we can plot them and evaluate our `peak voxels` based on their `distribution` of `residuals`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# colors for each of the clusters\n",
    "colors = ['blue', 'navy', 'purple', 'magenta', 'olive', 'teal']\n",
    "\n",
    "\n",
    "fig2, axs2 = plt.subplots(2, 3)\n",
    "axs2 = axs2.flatten()\n",
    "for i in range(0, 6):\n",
    "    axs2[i].set_title('Cluster peak {}\\n'.format(coords[i]))\n",
    "    axs2[i].hist(resid[:, i], color=colors[i])\n",
    "    print('Mean residuals: {}'.format(resid[:, i].mean()))\n",
    "\n",
    "fig2.set_size_inches(12, 7)\n",
    "fig2.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get and check predicted time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate the `predicted time series` we need to extract them, as well as the `actual time series`. To do so, we can use the `masker` again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_timeseries = masker.fit_transform(fmri_img)\n",
    "predicted_timeseries = masker.fit_transform(fmri_glm.predicted[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having obtained both `time series`, we can plot them against each other. To make it more informative, we will also visualize the respective `peak voxels` on the `mean functional image`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "\n",
    "# plot the time series and corresponding locations\n",
    "fig1, axs1 = plt.subplots(2, 6)\n",
    "for i in range(0, 6):\n",
    "    # plotting time series\n",
    "    axs1[0, i].set_title('Cluster peak {}\\n'.format(coords[i]))\n",
    "    axs1[0, i].plot(real_timeseries[:, i], c=colors[i], lw=2)\n",
    "    axs1[0, i].plot(predicted_timeseries[:, i], c='r', ls='--', lw=2)\n",
    "    axs1[0, i].set_xlabel('Time')\n",
    "    axs1[0, i].set_ylabel('Signal intensity', labelpad=0)\n",
    "    # plotting image below the time series\n",
    "    roi_img = plotting.plot_stat_map(\n",
    "        z_map, cut_coords=[coords[i][2]], threshold=3.1, figure=fig1,\n",
    "        axes=axs1[1, i], display_mode='z', colorbar=False, bg_img=mean_img, cmap='magma')\n",
    "    roi_img.add_markers([coords[i]], colors[i], 300)\n",
    "fig1.set_size_inches(24, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the R-squared\n",
    "Another option to evaluate our model is to plot the `R-squared`, that is the amount of variance explained through our `GLM` in total. While this plot will be informative, its interpretation will be limited as we can't tell if a voxel exhibits a large `R-squared` because of a response to a `condition` in our experiment or to `noise`. For these things, one should employ `F-Tests` as shown above. However, as expected we see that the `R-squared` decreases the further away `voxels` are from the `receive coils` (e.g. deeper in the brain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_stat_map(fmri_glm.r_square[0], bg_img=mean_img, threshold=.1,\n",
    "                       display_mode='z', cut_coords=7, cmap='magma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group level statistics\n",
    "Now that we've explored the individual level analysis quite a bit, one might ask: but what about `group level` statistics? No problem at all, `nilearn`'s `GLM` functionality of course supports this as well. As in other software packages, we need to repeat the `individual level analysis` for each subject to obtain the same contrast images, that we can submit to a `group level analysis`. \n",
    "\n",
    "### Run individual level analysis for multiple participants\n",
    "By now, we know how to do this easily. Let's use a simple `for loop` to repeat the analysis from above for `sub-02` and `sub-03`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in ['02', '03']:\n",
    "\n",
    "    # set the fMRI image\n",
    "    fmri_img = '/data/ds000114/derivatives/fmriprep/sub-%s/ses-test/func/sub-%s_ses-test_task-fingerfootlips_space-MNI152nlin2009casym_desc-preproc_bold.nii.gz' %(subject, subject)\n",
    "    \n",
    "    # read in the events \n",
    "    events = pd.read_table('/data/ds000114/sub-%s/ses-test/func/sub-%s_ses-test_task-fingerfootlips_events.tsv' %(subject, subject))\n",
    "    \n",
    "    # read in the confounds\n",
    "    confounds = pd.read_table('/data/ds000114/derivatives/fmriprep/sub-%s/ses-test/func/sub-%s_ses-test_task-fingerfootlips_bold_desc-confounds_timeseries.tsv' %(subject, subject))\n",
    "    \n",
    "    # restrict the to be included confounds to a subset\n",
    "    confounds_glm = confounds[['WhiteMatter', 'GlobalSignal', 'FramewiseDisplacement', 'X', 'Y', 'Z', 'RotX', 'RotY', 'RotZ']].replace(np.nan, 0)\n",
    "    \n",
    "    # run the GLM\n",
    "    fmri_glm = fmri_glm.fit(fmri_img, events, confounds_glm)\n",
    "    \n",
    "    # compute the contrast as a z-map\n",
    "    z_map = fmri_glm.compute_contrast(conditions['active - Finger'],\n",
    "                                  output_type='z_score')\n",
    "    \n",
    "    # save the z-map\n",
    "    z_map.to_filename(join(outdir, 'sub-%s_ses-test_task-footfingerlips_space-MNI152nlin2009casym_desc-finger_zmap.nii.gz' %subject))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a group level model\n",
    "As we now have the same contrast from multiple `subjects` we can define our `group level model`. At first, we need to gather the `individual contrast maps`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "list_z_maps = glob(join(outdir, 'sub-*_ses-test_task-footfingerlips_space-MNI152nlin2009casym_desc-finger_zmap.nii.gz'))\n",
    "\n",
    "list_z_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The next step includes the definition of a `design matrix`. As we want to run a simple `one-sample t-test`, we just need to indicate as many `1` as we have `z-maps`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_matrix = pd.DataFrame([1] * len(list_z_maps),\n",
    "                             columns=['intercept'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Believe it or not, that's all it takes. Within the next step we can already set and run our model. It's basically identical to the `First_level_model`: we need to define the `images` and `design matrix`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.glm.second_level import SecondLevelModel\n",
    "second_level_model = SecondLevelModel()\n",
    "second_level_model = second_level_model.fit(list_z_maps,\n",
    "                                            design_matrix=design_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same holds true for `contrast computation`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_map_group = second_level_model.compute_contrast(output_type='z_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we get? After defining a liberal threshold of `p<0.001 (uncorrected)`, we can plot our computed `group level contrast image`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "p001_unc = norm.isf(0.001)\n",
    "\n",
    "plotting.plot_glass_brain(z_map_group, colorbar=True, threshold=p001_unc,\n",
    "                          title='Group Finger tapping (unc p<0.001)',\n",
    "                          plot_abs=False, display_mode='x', cmap='magma')\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, not much going there...But please remember we also just included three participants. Besides this rather simple model, `nilearn`'s `GLM` functionality of course also allows you to run `paired t-test`, `two-sample t-test`, `F-test`, etc. . As shown above, you also can define different `thresholds` and `multiple comparison corrections`. There's yet another cool thing we didn't talk about. It's possible to run analyses in a rather automated way if your dataset is in `BIDS`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing statistical analyses on BIDS datasets\n",
    "Even though model specification and running was comparably easy and straightforward, it can be even better. `Nilearn`'s `GLM` functionality actually enables you to define models for multiple participants through one function by leveraging the `BIDS` standard. More precisely, the function `first_level_from_bids` takes the same input arguments as `First_Level_model` (e.g. `t_r`, `hrf_model`, `high_pass`, etc.), but through defining the `BIDS raw` and `derivatives folder`, as well as a `task` and `space` label automatically extracts all information necessary to run `individual level models` and creates the `model` itself for all participants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nilearn.glm.first_level import first_level_from_bids\n",
    "\n",
    "data_dir = '/data/ds000114/'\n",
    "task_label = 'fingerfootlips'\n",
    "space_label = 'MNI152nlin2009casym'\n",
    "derivatives_folder = 'derivatives/fmriprep'\n",
    "\n",
    "models, models_run_imgs, models_events, models_confounds = \\\n",
    "    first_level_from_bids(data_dir, task_label, space_label,\n",
    "                          smoothing_fwhm=5.0,\n",
    "                          derivatives_folder=derivatives_folder, \n",
    "                          t_r=2.5, \n",
    "                          noise_model='ar1',\n",
    "                          hrf_model='spm',\n",
    "                          drift_model='cosine',\n",
    "                          high_pass=1./160,\n",
    "                          signal_scaling=False,\n",
    "                          minimize_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done, let's check if things work as expected. As an example, we will have a look at the information for `sub-01`. We're going to start with the `images`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print([os.path.basename(run) for run in models_run_imgs[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. How about confounds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models_confounds[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, the `NaN` again. Let's fix those as we did last time, but for all participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_confounds_no_nan = []\n",
    "\n",
    "for confounds in models_confounds:\n",
    "    models_confounds_no_nan.append(confounds[0].fillna(0)[['WhiteMatter', 'GlobalSignal', 'FramewiseDisplacement', 'X', 'Y', 'Z', 'RotX', 'RotY', 'RotZ']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least: how do the `events` look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models_events[0][0]['trial_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic, now we're ready to run our models. With a little `zip` magic this is done without a problem. We also going to compute `z-maps` as before and plot them side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models_fitted = [] \n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 8.5))\n",
    "model_and_args = zip(models, models_run_imgs, models_events, models_confounds_no_nan)\n",
    "for midx, (model, imgs, events, confounds) in enumerate(model_and_args):\n",
    "    # fit the GLM\n",
    "    model.fit(imgs, events, confounds)\n",
    "        \n",
    "    models_fitted.append(model)\n",
    "    \n",
    "    # compute the contrast of interest\n",
    "    zmap = model.compute_contrast('Finger')\n",
    "    plotting.plot_glass_brain(zmap, colorbar=False, threshold=p001_unc,\n",
    "                              title=('sub-' + model.subject_label),\n",
    "                              axes=axes[int(midx)-1],\n",
    "                              plot_abs=False, display_mode='x', cmap='magma')\n",
    "fig.suptitle('subjects z_map finger tapping (unc p<0.001)')\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks about right. However, let's also check the `design matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_design_matrix\n",
    "plot_design_matrix(models_fitted[0].design_matrices_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and `contrast matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contrast_matrix('Finger', models_fitted[0].design_matrices_[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing to complain here and thus we can move on to the `group level model`. Instead of assembling `contrast images` from each participant, we also have the option to simply provide the `fitted individual level models` as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.glm.second_level import SecondLevelModel\n",
    "second_level_input = models_fitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all it takes and we can run our `group level model` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_level_model = SecondLevelModel()\n",
    "second_level_model = second_level_model.fit(second_level_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And after computing the `contrast`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zmap = second_level_model.compute_contrast(\n",
    "    first_level_contrast='Finger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can plot the results again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_glass_brain(zmap, colorbar=True, threshold=p001_unc,\n",
    "                          title='Group Finger tapping (unc p<0.001)',\n",
    "                          plot_abs=False, display_mode='x', cmap='magma')\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all for now. Please note, that we only showed a small part of what's possible. Make sure to check the documentation and the examples it includes. We hope we could show you how powerful `nilearn` will be through including `GLM` functionality  starting with the new release. While there's already a lot you can do, there will be even more in the future. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
